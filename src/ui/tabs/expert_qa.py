import streamlit as st

def render_expert_qa_tab():
    """Render the Expert QA / Chat with Doctor tab."""    
    # --- Configuration Section ---
    with st.expander("âš™ï¸ ä¸“å®¶è®¾å®šä¸çŸ¥è¯†åº“é…ç½®", expanded=True):
        col1, col2 = st.columns(2)
        
        with col1:
            st.subheader("ğŸ‘¨â€âš•ï¸ åŒ»ç”Ÿäººè®¾")
            default_prompt = "ä½ æ˜¯ä¸€ä¸ªç»éªŒä¸°å¯Œçš„å…¨ç§‘åŒ»ç”Ÿã€‚"
            if "qa_system_prompt" not in st.session_state:
                st.session_state.qa_system_prompt = default_prompt
                
            st.session_state.qa_system_prompt = st.text_area(
                "System Instruction",
                value=st.session_state.qa_system_prompt,
                height=150,
                key="qa_sys_prompt_input"
            )
        
        with col2:
            st.subheader("ğŸ“š RAG çŸ¥è¯†åº“")
            
            # List existing KBs
            existing_kbs = st.session_state.rag_manager.list_knowledge_bases()
            
            kb_mode = st.radio("çŸ¥è¯†åº“æ“ä½œ", ["é€‰æ‹©ç°æœ‰çŸ¥è¯†åº“", "æ–°å»º/æ›´æ–°çŸ¥è¯†åº“"], horizontal=True)
            
            if kb_mode == "é€‰æ‹©ç°æœ‰çŸ¥è¯†åº“":
                if existing_kbs:
                    selected_kb = st.selectbox("é€‰æ‹©çŸ¥è¯†åº“", existing_kbs, key="qa_kb_selector")
                    if selected_kb != st.session_state.rag_manager.current_kb_name:
                         if st.button("ğŸ“‚ åŠ è½½è¯¥çŸ¥è¯†åº“"):
                            with st.spinner(f"æ­£åœ¨åŠ è½½ {selected_kb}..."):
                                if st.session_state.rag_manager.load_knowledge_base(selected_kb):
                                    st.success(f"å·²åŠ è½½: {selected_kb}")
                                else:
                                    st.error("åŠ è½½å¤±è´¥")
                else:
                    st.info("æš‚æ— æœ¬åœ°çŸ¥è¯†åº“ï¼Œè¯·å…ˆæ–°å»ºã€‚")
            
            else: # Create New
                new_kb_name = st.text_input("çŸ¥è¯†åº“åç§° (è‹±æ–‡/æ•°å­—)", placeholder="e.g. pediatrics_v1")
                uploaded_files = st.file_uploader(
                    "ä¸Šä¼ å‚è€ƒæ–‡æ¡£ (TXT/MD)", 
                    accept_multiple_files=True,
                    type=["txt", "md"],
                    key="qa_file_uploader"
                )
                
                if uploaded_files and new_kb_name:
                    if st.button("ğŸš€ åˆ›å»ºå¹¶å¤„ç†"):
                        with st.spinner("æ­£åœ¨å¤„ç†æ–‡æ¡£å¹¶æ„å»ºç´¢å¼•..."):
                            status = st.session_state.rag_manager.process_files(new_kb_name, uploaded_files)
                            st.success(status)
                            st.rerun() # Refresh to show in list

            st.caption(f"å½“å‰çŠ¶æ€: {st.session_state.rag_manager.vector_store_status}")
            
            enable_rag = st.checkbox("å¼€å¯ RAG æ¨¡å¼", value=getattr(st.session_state, 'enable_rag', False), key="qa_enable_rag")
            st.session_state.enable_rag = enable_rag
            
            if enable_rag:
                st.session_state.rag_threshold = st.slider("ç›¸ä¼¼åº¦é˜ˆå€¼", 0.0, 1.0, 0.6, key="qa_rag_threshold")
                st.session_state.rag_top_k = st.slider("æ£€ç´¢ Top-K", 1, 10, 3, key="qa_rag_topk")

    st.markdown("---")

    # Quick Questions and Reset
    if not st.session_state.messages_qa:
        col_q1, col_q2, col_q3 = st.columns(3)
        with col_q1:
            if st.button("ä¸´åºŠè¯ç†å­¦çš„ä¸»è¦ç ”ç©¶å†…å®¹å’Œæ ¸å¿ƒä»»åŠ¡åˆ†åˆ«æ˜¯ä»€ä¹ˆ?"):
                st.session_state.current_input = "ä¸´åºŠè¯ç†å­¦çš„ä¸»è¦ç ”ç©¶å†…å®¹å’Œæ ¸å¿ƒä»»åŠ¡åˆ†åˆ«æ˜¯ä»€ä¹ˆ?"
        with col_q2:
            if st.button("æ–°è¯ä¸´åºŠè¯•éªŒçš„ Iã€IIã€IIIã€IV æœŸå„è‡ªçš„ä¸»è¦å†…å®¹æ˜¯ä»€ä¹ˆ"):
                st.session_state.current_input = "æ–°è¯ä¸´åºŠè¯•éªŒçš„ Iã€IIã€IIIã€IV æœŸå„è‡ªçš„ä¸»è¦å†…å®¹æ˜¯ä»€ä¹ˆ"
        with col_q3:
            if st.button("ä¸´åºŠè¯•éªŒä¸­å¿…é¡»éµå¾ªå“ªäº›æ ¸å¿ƒä¼¦ç†å­¦åŸåˆ™ï¼Ÿ"):
                st.session_state.current_input = "ä¸´åºŠè¯•éªŒä¸­å¿…é¡»éµå¾ªå“ªäº›æ ¸å¿ƒä¼¦ç†å­¦åŸåˆ™ï¼Ÿ"
    else:
        if st.button("ğŸ”„ é‡ç½®å¯¹è¯"):
            st.session_state.messages_qa = []
            st.rerun()

    # Chat History
    for msg in st.session_state.messages_qa:
        with st.chat_message(msg["role"]):
            st.write(msg["content"])
            
            # Show RAG context if enabled (even if empty)
            if msg["role"] == "assistant" and getattr(st.session_state, 'enable_rag', False):
                rag_data = msg.get("rag_context", [])
                
                with st.expander(f"ğŸ“š å‚è€ƒäº† {len(rag_data)} ä¸ªæ–‡æ¡£ç‰‡æ®µ"):
                    if not rag_data:
                        st.caption("æ²¡æœ‰æ‰¾åˆ°ç¬¦åˆé˜ˆå€¼çš„ç›¸å…³æ–‡æ¡£ã€‚")
                    else:
                        for idx, ctx in enumerate(rag_data):
                            score = float(ctx.get('similarity', 0.0))
                            text = ctx.get('text', '')
                            # Show a snippet in the header
                            summary = f"ç‰‡æ®µ {idx+1} (ç›¸ä¼¼åº¦: {score:.4f})"
                            
                            # Use HTML details for nested expander effect
                            st.markdown(
                                f"""
                                <details>
                                <summary>{summary}</summary>
                                <div style='padding: 10px; border-left: 3px solid #ccc; background-color: #f9f9f9; margin-top: 5px;'>
                                <pre style='white-space: pre-wrap; word-wrap: break-word;'>{text}</pre>
                                </div>
                                </details>
                                """,
                                unsafe_allow_html=True
                            )

    # Input Area
    if "current_input" in st.session_state and st.session_state.current_input:
        prompt = st.session_state.current_input
        del st.session_state.current_input
        handle_user_input(prompt)
        st.rerun()
    
    prompt = st.chat_input("è¯·è¾“å…¥æ‚¨çš„ç—…æƒ…æˆ–é—®é¢˜...")
    if prompt:
        handle_user_input(prompt)
        st.rerun()

def handle_user_input(prompt: str):
    """Process user input for QA tab."""
    st.session_state.messages_qa.append({"role": "user", "content": prompt})
    
    with st.chat_message("user"):
        st.write(prompt)

    # 1. RAG Retrieval
    rag_context = []
    if getattr(st.session_state, 'enable_rag', False):
         rag_context = st.session_state.rag_manager.retrieve(
             prompt, 
             getattr(st.session_state, 'rag_threshold', 0.7), 
             getattr(st.session_state, 'rag_top_k', 3)
         )
    
    # 2. Build Prompt with Context
    full_prompt = prompt
    if rag_context:
        # Extract text for the prompt
        context_str = "\n".join([item['text'] for item in rag_context])
        full_prompt = f"Background Information:\n{context_str}\n\nUser Question: {prompt}"
        
    # 3. Call Camel Agent with Streaming
    response_content = ""
    with st.chat_message("assistant"):
        with st.spinner("åŒ»ç”Ÿæ­£åœ¨æ€è€ƒ..."):
            from camel.agents import ChatAgent
            from camel.messages import BaseMessage
            from camel.models import ModelFactory
            from camel.types import ModelPlatformType
            
            # Helper to create model
            model_config = st.session_state.model_config
            model_instance = ModelFactory.create(
                model_platform=ModelPlatformType.OPENAI,
                model_type=model_config.model_name or "qwen-plus",
                url=model_config.base_url,
                api_key=model_config.api_key,
                model_config_dict={"temperature": model_config.temperature}
            )
            
            # System Message
            sys_msg = BaseMessage.make_assistant_message(
                role_name="Expert",
                content=st.session_state.qa_system_prompt
            )
            
            agent = ChatAgent(system_message=sys_msg, model=model_instance)
            
            user_msg = BaseMessage.make_user_message(role_name="User", content=full_prompt)
            
            try:
                response = agent.step(user_msg)
                response_content = response.msg.content if response and getattr(response, "msg", None) else ""
            except Exception as exc:
                response_content = f"æ¨¡å‹å“åº”å¤±è´¥ï¼š{exc}"
            
            st.write(response_content or "ï¼ˆæœªè¿”å›å†…å®¹ï¼‰")

    st.session_state.messages_qa.append({
        "role": "assistant", 
        "content": response_content,
        "rag_context": rag_context
    })
